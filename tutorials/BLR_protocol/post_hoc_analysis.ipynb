{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnjOHEPnSgqZ"
   },
   "source": [
    "# SVM classification SZ vs. HC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1dgnArWand-"
   },
   "source": [
    "Classify schizophrenia group from controls using cortical thickness deviation scores (z-scores) and then the true cortical thickness data to see which type of data better separates the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oer08RX7Sgqc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "os.chdir('/content/PCNtoolkit-demo/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBA6wv5_Sgqd"
   },
   "outputs": [],
   "source": [
    "Z_df = pd.read_csv('data/fcon1000_te_Z.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AtT_a9QSgqe"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0m3frZSqWHFt"
   },
   "outputs": [],
   "source": [
    "Z_df.dropna(subset=['group'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reWNrhN6Wge0"
   },
   "outputs": [],
   "source": [
    "Z_df['group'] = Z_df['group'].replace(\"SZ\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuddguUsW_UI"
   },
   "outputs": [],
   "source": [
    "Z_df['group'] = Z_df['group'].replace(\"Control\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBuQvJKqVz0p"
   },
   "outputs": [],
   "source": [
    "deviations = Z_df.loc[:, Z_df.columns.str.contains('Z_predict')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZvu0iXlZg7P"
   },
   "outputs": [],
   "source": [
    "cortical_thickness = Z_df.loc[:, Z_df.columns.str.endswith('_thickness')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HECqM4rZVcD9"
   },
   "outputs": [],
   "source": [
    "# Data IO and generation\n",
    "X1 = deviations\n",
    "X2 = cortical_thickness\n",
    "y = Z_df['group']\n",
    "n_samples, n_features = X1.shape\n",
    "random_state = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iho4wkAESgqf"
   },
   "outputs": [],
   "source": [
    "X1 = X1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zi7v5e8vZ0Ms"
   },
   "outputs": [],
   "source": [
    "X2 = X2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcA4w73TSgqf"
   },
   "outputs": [],
   "source": [
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKcM-dA3ZG_u"
   },
   "outputs": [],
   "source": [
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deviation scores as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNRcb-pvSgqf"
   },
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Classification and ROC analysis\n",
    "\n",
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "classifier = svm.SVC(kernel='linear', probability=True,\n",
    "                     random_state=random_state)\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "parameters = {'axes.labelsize': 20,\n",
    "          'axes.titlesize': 25, 'xtick.labelsize':16,'ytick.labelsize':16,'legend.fontsize':14,'legend.title_fontsize':16}\n",
    "plt.rcParams.update(parameters)\n",
    "\n",
    "for i, (train, test) in enumerate(cv.split(X1, y)):\n",
    "    classifier.fit(X1[train], y[train])\n",
    "    viz = plot_roc_curve(classifier, X1[test], y[test],\n",
    "                         name='ROC fold {}'.format(i),\n",
    "                         alpha=0.3, lw=1, ax=ax)\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "        label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax.set_title('Receiver operating characteristic SZ vs. HC (deviations)', fontweight=\"bold\", size=20)\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Raw cortical thickness data as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYPilmZOaNgs"
   },
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Classification and ROC analysis\n",
    "\n",
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "classifier = svm.SVC(kernel='linear', probability=True,\n",
    "                     random_state=random_state)\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "parameters = {'axes.labelsize': 20,\n",
    "          'axes.titlesize': 25, 'xtick.labelsize':16,'ytick.labelsize':16,'legend.fontsize':14,'legend.title_fontsize':16}\n",
    "plt.rcParams.update(parameters)\n",
    "\n",
    "for i, (train, test) in enumerate(cv.split(X2, y)):\n",
    "    classifier.fit(X2[train], y[train])\n",
    "    viz = plot_roc_curve(classifier, X2[test], y[test],\n",
    "                         name='ROC fold {}'.format(i),\n",
    "                         alpha=0.3, lw=1, ax=ax)\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "        label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax.set_title('Receiver operating characteristic SZ vs. HC (cortical thickness)', fontweight=\"bold\", size=20)\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9iIgxR1YMzq"
   },
   "source": [
    "Which brain feature leads to a better classification between SZ & HC? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical case-control testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Multiple Testing and P-Value Correction\n",
    "\n",
    "\n",
    "Author: Josef Perktold\n",
    "License: BSD-3\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.stats._knockoff import RegressionFDR\n",
    "\n",
    "__all__ = ['fdrcorrection', 'fdrcorrection_twostage', 'local_fdr',\n",
    "           'multipletests', 'NullDistribution', 'RegressionFDR']\n",
    "\n",
    "# ==============================================\n",
    "#\n",
    "# Part 1: Multiple Tests and P-Value Correction\n",
    "#\n",
    "# ==============================================\n",
    "\n",
    "\n",
    "def _ecdf(x):\n",
    "    '''no frills empirical cdf used in fdrcorrection\n",
    "    '''\n",
    "    nobs = len(x)\n",
    "    return np.arange(1,nobs+1)/float(nobs)\n",
    "\n",
    "multitest_methods_names = {'b': 'Bonferroni',\n",
    "                           's': 'Sidak',\n",
    "                           'h': 'Holm',\n",
    "                           'hs': 'Holm-Sidak',\n",
    "                           'sh': 'Simes-Hochberg',\n",
    "                           'ho': 'Hommel',\n",
    "                           'fdr_bh': 'FDR Benjamini-Hochberg',\n",
    "                           'fdr_by': 'FDR Benjamini-Yekutieli',\n",
    "                           'fdr_tsbh': 'FDR 2-stage Benjamini-Hochberg',\n",
    "                           'fdr_tsbky': 'FDR 2-stage Benjamini-Krieger-Yekutieli',\n",
    "                           'fdr_gbs': 'FDR adaptive Gavrilov-Benjamini-Sarkar'\n",
    "                           }\n",
    "\n",
    "_alias_list = [['b', 'bonf', 'bonferroni'],\n",
    "               ['s', 'sidak'],\n",
    "               ['h', 'holm'],\n",
    "               ['hs', 'holm-sidak'],\n",
    "               ['sh', 'simes-hochberg'],\n",
    "               ['ho', 'hommel'],\n",
    "               ['fdr_bh', 'fdr_i', 'fdr_p', 'fdri', 'fdrp'],\n",
    "               ['fdr_by', 'fdr_n', 'fdr_c', 'fdrn', 'fdrcorr'],\n",
    "               ['fdr_tsbh', 'fdr_2sbh'],\n",
    "               ['fdr_tsbky', 'fdr_2sbky', 'fdr_twostage'],\n",
    "               ['fdr_gbs']\n",
    "               ]\n",
    "\n",
    "\n",
    "multitest_alias = {}\n",
    "for m in _alias_list:\n",
    "    multitest_alias[m[0]] = m[0]\n",
    "    for a in m[1:]:\n",
    "        multitest_alias[a] = m[0]\n",
    "\n",
    "def multipletests(pvals, alpha=0.05, method='hs', is_sorted=False,\n",
    "                  returnsorted=False):\n",
    "    \"\"\"\n",
    "    Test results and p-value correction for multiple tests\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pvals : array_like, 1-d\n",
    "        uncorrected p-values.   Must be 1-dimensional.\n",
    "    alpha : float\n",
    "        FWER, family-wise error rate, e.g. 0.1\n",
    "    method : str\n",
    "        Method used for testing and adjustment of pvalues. Can be either the\n",
    "        full name or initial letters. Available methods are:\n",
    "\n",
    "        - `bonferroni` : one-step correction\n",
    "        - `sidak` : one-step correction\n",
    "        - `holm-sidak` : step down method using Sidak adjustments\n",
    "        - `holm` : step-down method using Bonferroni adjustments\n",
    "        - `simes-hochberg` : step-up method  (independent)\n",
    "        - `hommel` : closed method based on Simes tests (non-negative)\n",
    "        - `fdr_bh` : Benjamini/Hochberg  (non-negative)\n",
    "        - `fdr_by` : Benjamini/Yekutieli (negative)\n",
    "        - `fdr_tsbh` : two stage fdr correction (non-negative)\n",
    "        - `fdr_tsbky` : two stage fdr correction (non-negative)\n",
    "\n",
    "    is_sorted : bool\n",
    "        If False (default), the p_values will be sorted, but the corrected\n",
    "        pvalues are in the original order. If True, then it assumed that the\n",
    "        pvalues are already sorted in ascending order.\n",
    "    returnsorted : bool\n",
    "         not tested, return sorted p-values instead of original sequence\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reject : ndarray, boolean\n",
    "        true for hypothesis that can be rejected for given alpha\n",
    "    pvals_corrected : ndarray\n",
    "        p-values corrected for multiple tests\n",
    "    alphacSidak : float\n",
    "        corrected alpha for Sidak method\n",
    "    alphacBonf : float\n",
    "        corrected alpha for Bonferroni method\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    There may be API changes for this function in the future.\n",
    "\n",
    "    Except for 'fdr_twostage', the p-value correction is independent of the\n",
    "    alpha specified as argument. In these cases the corrected p-values\n",
    "    can also be compared with a different alpha. In the case of 'fdr_twostage',\n",
    "    the corrected p-values are specific to the given alpha, see\n",
    "    ``fdrcorrection_twostage``.\n",
    "\n",
    "    The 'fdr_gbs' procedure is not verified against another package, p-values\n",
    "    are derived from scratch and are not derived in the reference. In Monte\n",
    "    Carlo experiments the method worked correctly and maintained the false\n",
    "    discovery rate.\n",
    "\n",
    "    All procedures that are included, control FWER or FDR in the independent\n",
    "    case, and most are robust in the positively correlated case.\n",
    "\n",
    "    `fdr_gbs`: high power, fdr control for independent case and only small\n",
    "    violation in positively correlated case\n",
    "\n",
    "    **Timing**:\n",
    "\n",
    "    Most of the time with large arrays is spent in `argsort`. When\n",
    "    we want to calculate the p-value for several methods, then it is more\n",
    "    efficient to presort the pvalues, and put the results back into the\n",
    "    original order outside of the function.\n",
    "\n",
    "    Method='hommel' is very slow for large arrays, since it requires the\n",
    "    evaluation of n partitions, where n is the number of p-values.\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    pvals = np.asarray(pvals)\n",
    "    alphaf = alpha  # Notation ?\n",
    "\n",
    "    if not is_sorted:\n",
    "        sortind = np.argsort(pvals)\n",
    "        pvals = np.take(pvals, sortind)\n",
    "\n",
    "    ntests = len(pvals)\n",
    "    alphacSidak = 1 - np.power((1. - alphaf), 1./ntests)\n",
    "    alphacBonf = alphaf / float(ntests)\n",
    "    if method.lower() in ['b', 'bonf', 'bonferroni']:\n",
    "        reject = pvals <= alphacBonf\n",
    "        pvals_corrected = pvals * float(ntests)\n",
    "\n",
    "    elif method.lower() in ['s', 'sidak']:\n",
    "        reject = pvals <= alphacSidak\n",
    "        pvals_corrected = 1 - np.power((1. - pvals), ntests)\n",
    "\n",
    "    elif method.lower() in ['hs', 'holm-sidak']:\n",
    "        alphacSidak_all = 1 - np.power((1. - alphaf),\n",
    "                                       1./np.arange(ntests, 0, -1))\n",
    "        notreject = pvals > alphacSidak_all\n",
    "        del alphacSidak_all\n",
    "\n",
    "        nr_index = np.nonzero(notreject)[0]\n",
    "        if nr_index.size == 0:\n",
    "            # nonreject is empty, all rejected\n",
    "            notrejectmin = len(pvals)\n",
    "        else:\n",
    "            notrejectmin = np.min(nr_index)\n",
    "        notreject[notrejectmin:] = True\n",
    "        reject = ~notreject\n",
    "        del notreject\n",
    "\n",
    "        pvals_corrected_raw = 1 - np.power((1. - pvals),\n",
    "                                           np.arange(ntests, 0, -1))\n",
    "        pvals_corrected = np.maximum.accumulate(pvals_corrected_raw)\n",
    "        del pvals_corrected_raw\n",
    "\n",
    "    elif method.lower() in ['h', 'holm']:\n",
    "        notreject = pvals > alphaf / np.arange(ntests, 0, -1)\n",
    "        nr_index = np.nonzero(notreject)[0]\n",
    "        if nr_index.size == 0:\n",
    "            # nonreject is empty, all rejected\n",
    "            notrejectmin = len(pvals)\n",
    "        else:\n",
    "            notrejectmin = np.min(nr_index)\n",
    "        notreject[notrejectmin:] = True\n",
    "        reject = ~notreject\n",
    "        pvals_corrected_raw = pvals * np.arange(ntests, 0, -1)\n",
    "        pvals_corrected = np.maximum.accumulate(pvals_corrected_raw)\n",
    "        del pvals_corrected_raw\n",
    "        gc.collect()\n",
    "\n",
    "    elif method.lower() in ['sh', 'simes-hochberg']:\n",
    "        alphash = alphaf / np.arange(ntests, 0, -1)\n",
    "        reject = pvals <= alphash\n",
    "        rejind = np.nonzero(reject)\n",
    "        if rejind[0].size > 0:\n",
    "            rejectmax = np.max(np.nonzero(reject))\n",
    "            reject[:rejectmax] = True\n",
    "        pvals_corrected_raw = np.arange(ntests, 0, -1) * pvals\n",
    "        pvals_corrected = np.minimum.accumulate(pvals_corrected_raw[::-1])[::-1]\n",
    "        del pvals_corrected_raw\n",
    "\n",
    "    elif method.lower() in ['ho', 'hommel']:\n",
    "        # we need a copy because we overwrite it in a loop\n",
    "        a = pvals.copy()\n",
    "        for m in range(ntests, 1, -1):\n",
    "            cim = np.min(m * pvals[-m:] / np.arange(1,m+1.))\n",
    "            a[-m:] = np.maximum(a[-m:], cim)\n",
    "            a[:-m] = np.maximum(a[:-m], np.minimum(m * pvals[:-m], cim))\n",
    "        pvals_corrected = a\n",
    "        reject = a <= alphaf\n",
    "\n",
    "    elif method.lower() in ['fdr_bh', 'fdr_i', 'fdr_p', 'fdri', 'fdrp']:\n",
    "        # delegate, call with sorted pvals\n",
    "        reject, pvals_corrected = fdrcorrection(pvals, alpha=alpha,\n",
    "                                                 method='indep',\n",
    "                                                 is_sorted=True)\n",
    "    elif method.lower() in ['fdr_by', 'fdr_n', 'fdr_c', 'fdrn', 'fdrcorr']:\n",
    "        # delegate, call with sorted pvals\n",
    "        reject, pvals_corrected = fdrcorrection(pvals, alpha=alpha,\n",
    "                                                 method='n',\n",
    "                                                 is_sorted=True)\n",
    "    elif method.lower() in ['fdr_tsbky', 'fdr_2sbky', 'fdr_twostage']:\n",
    "        # delegate, call with sorted pvals\n",
    "        reject, pvals_corrected = fdrcorrection_twostage(pvals, alpha=alpha,\n",
    "                                                         method='bky',\n",
    "                                                         is_sorted=True)[:2]\n",
    "    elif method.lower() in ['fdr_tsbh', 'fdr_2sbh']:\n",
    "        # delegate, call with sorted pvals\n",
    "        reject, pvals_corrected = fdrcorrection_twostage(pvals, alpha=alpha,\n",
    "                                                         method='bh',\n",
    "                                                         is_sorted=True)[:2]\n",
    "\n",
    "    elif method.lower() in ['fdr_gbs']:\n",
    "        #adaptive stepdown in Gavrilov, Benjamini, Sarkar, Annals of Statistics 2009\n",
    "##        notreject = pvals > alphaf / np.arange(ntests, 0, -1) #alphacSidak\n",
    "##        notrejectmin = np.min(np.nonzero(notreject))\n",
    "##        notreject[notrejectmin:] = True\n",
    "##        reject = ~notreject\n",
    "\n",
    "        ii = np.arange(1, ntests + 1)\n",
    "        q = (ntests + 1. - ii)/ii * pvals / (1. - pvals)\n",
    "        pvals_corrected_raw = np.maximum.accumulate(q) #up requirementd\n",
    "\n",
    "        pvals_corrected = np.minimum.accumulate(pvals_corrected_raw[::-1])[::-1]\n",
    "        del pvals_corrected_raw\n",
    "        reject = pvals_corrected <= alpha\n",
    "\n",
    "    else:\n",
    "        raise ValueError('method not recognized')\n",
    "\n",
    "    if pvals_corrected is not None: #not necessary anymore\n",
    "        pvals_corrected[pvals_corrected>1] = 1\n",
    "    if is_sorted or returnsorted:\n",
    "        return reject, pvals_corrected, alphacSidak, alphacBonf\n",
    "    else:\n",
    "        pvals_corrected_ = np.empty_like(pvals_corrected)\n",
    "        pvals_corrected_[sortind] = pvals_corrected\n",
    "        del pvals_corrected\n",
    "        reject_ = np.empty_like(reject)\n",
    "        reject_[sortind] = reject\n",
    "        return reject_, pvals_corrected_, alphacSidak, alphacBonf\n",
    "\n",
    "\n",
    "def fdrcorrection(pvals, alpha=0.05, method='indep', is_sorted=False):\n",
    "    '''pvalue correction for false discovery rate\n",
    "\n",
    "    This covers Benjamini/Hochberg for independent or positively correlated and\n",
    "    Benjamini/Yekutieli for general or negatively correlated tests. Both are\n",
    "    available in the function multipletests, as method=`fdr_bh`, resp. `fdr_by`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pvals : array_like\n",
    "        set of p-values of the individual tests.\n",
    "    alpha : float\n",
    "        error rate\n",
    "    method : {'indep', 'negcorr'}\n",
    "    is_sorted : bool\n",
    "        If False (default), the p_values will be sorted, but the corrected\n",
    "        pvalues are in the original order. If True, then it assumed that the\n",
    "        pvalues are already sorted in ascending order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rejected : ndarray, bool\n",
    "        True if a hypothesis is rejected, False if not\n",
    "    pvalue-corrected : ndarray\n",
    "        pvalues adjusted for multiple hypothesis testing to limit FDR\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    If there is prior information on the fraction of true hypothesis, then alpha\n",
    "    should be set to alpha * m/m_0 where m is the number of tests,\n",
    "    given by the p-values, and m_0 is an estimate of the true hypothesis.\n",
    "    (see Benjamini, Krieger and Yekuteli)\n",
    "\n",
    "    The two-step method of Benjamini, Krieger and Yekutiel that estimates the number\n",
    "    of false hypotheses will be available (soon).\n",
    "\n",
    "    Method names can be abbreviated to first letter, 'i' or 'p' for fdr_bh and 'n' for\n",
    "    fdr_by.\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    pvals = np.asarray(pvals)\n",
    "\n",
    "    if not is_sorted:\n",
    "        pvals_sortind = np.argsort(pvals)\n",
    "        pvals_sorted = np.take(pvals, pvals_sortind)\n",
    "    else:\n",
    "        pvals_sorted = pvals  # alias\n",
    "\n",
    "    if method in ['i', 'indep', 'p', 'poscorr']:\n",
    "        ecdffactor = _ecdf(pvals_sorted)\n",
    "    elif method in ['n', 'negcorr']:\n",
    "        cm = np.sum(1./np.arange(1, len(pvals_sorted)+1))   #corrected this\n",
    "        ecdffactor = _ecdf(pvals_sorted) / cm\n",
    "##    elif method in ['n', 'negcorr']:\n",
    "##        cm = np.sum(np.arange(len(pvals)))\n",
    "##        ecdffactor = ecdf(pvals_sorted)/cm\n",
    "    else:\n",
    "        raise ValueError('only indep and negcorr implemented')\n",
    "    reject = pvals_sorted <= ecdffactor*alpha\n",
    "    if reject.any():\n",
    "        rejectmax = max(np.nonzero(reject)[0])\n",
    "        reject[:rejectmax] = True\n",
    "\n",
    "    pvals_corrected_raw = pvals_sorted / ecdffactor\n",
    "    pvals_corrected = np.minimum.accumulate(pvals_corrected_raw[::-1])[::-1]\n",
    "    del pvals_corrected_raw\n",
    "    pvals_corrected[pvals_corrected>1] = 1\n",
    "    if not is_sorted:\n",
    "        pvals_corrected_ = np.empty_like(pvals_corrected)\n",
    "        pvals_corrected_[pvals_sortind] = pvals_corrected\n",
    "        del pvals_corrected\n",
    "        reject_ = np.empty_like(reject)\n",
    "        reject_[pvals_sortind] = reject\n",
    "        return reject_, pvals_corrected_\n",
    "    else:\n",
    "        return reject, pvals_corrected\n",
    "\n",
    "\n",
    "def fdrcorrection_twostage(pvals, alpha=0.05, method='bky', iter=False,\n",
    "                           is_sorted=False):\n",
    "    '''(iterated) two stage linear step-up procedure with estimation of number of true\n",
    "    hypotheses\n",
    "\n",
    "    Benjamini, Krieger and Yekuteli, procedure in Definition 6\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pvals : array_like\n",
    "        set of p-values of the individual tests.\n",
    "    alpha : float\n",
    "        error rate\n",
    "    method : {'bky', 'bh')\n",
    "        see Notes for details\n",
    "\n",
    "        * 'bky' - implements the procedure in Definition 6 of Benjamini, Krieger\n",
    "           and Yekuteli 2006\n",
    "        * 'bh' - the two stage method of Benjamini and Hochberg\n",
    "\n",
    "    iter : bool\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rejected : ndarray, bool\n",
    "        True if a hypothesis is rejected, False if not\n",
    "    pvalue-corrected : ndarray\n",
    "        pvalues adjusted for multiple hypotheses testing to limit FDR\n",
    "    m0 : int\n",
    "        ntest - rej, estimated number of true hypotheses\n",
    "    alpha_stages : list of floats\n",
    "        A list of alphas that have been used at each stage\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The returned corrected p-values are specific to the given alpha, they\n",
    "    cannot be used for a different alpha.\n",
    "\n",
    "    The returned corrected p-values are from the last stage of the fdr_bh\n",
    "    linear step-up procedure (fdrcorrection0 with method='indep') corrected\n",
    "    for the estimated fraction of true hypotheses.\n",
    "    This means that the rejection decision can be obtained with\n",
    "    ``pval_corrected <= alpha``, where ``alpha`` is the original significance\n",
    "    level.\n",
    "    (Note: This has changed from earlier versions (<0.5.0) of statsmodels.)\n",
    "\n",
    "    BKY described several other multi-stage methods, which would be easy to implement.\n",
    "    However, in their simulation the simple two-stage method (with iter=False) was the\n",
    "    most robust to the presence of positive correlation\n",
    "\n",
    "    TODO: What should be returned?\n",
    "\n",
    "    '''\n",
    "    pvals = np.asarray(pvals)\n",
    "\n",
    "    if not is_sorted:\n",
    "        pvals_sortind = np.argsort(pvals)\n",
    "        pvals = np.take(pvals, pvals_sortind)\n",
    "\n",
    "    ntests = len(pvals)\n",
    "    if method == 'bky':\n",
    "        fact = (1.+alpha)\n",
    "        alpha_prime = alpha / fact\n",
    "    elif method == 'bh':\n",
    "        fact = 1.\n",
    "        alpha_prime = alpha\n",
    "    else:\n",
    "        raise ValueError(\"only 'bky' and 'bh' are available as method\")\n",
    "\n",
    "    alpha_stages = [alpha_prime]\n",
    "    rej, pvalscorr = fdrcorrection(pvals, alpha=alpha_prime, method='indep',\n",
    "                                   is_sorted=True)\n",
    "    r1 = rej.sum()\n",
    "    if (r1 == 0) or (r1 == ntests):\n",
    "        return rej, pvalscorr * fact, ntests - r1, alpha_stages\n",
    "    ri_old = r1\n",
    "\n",
    "    while True:\n",
    "        ntests0 = 1.0 * ntests - ri_old\n",
    "        alpha_star = alpha_prime * ntests / ntests0\n",
    "        alpha_stages.append(alpha_star)\n",
    "        #print ntests0, alpha_star\n",
    "        rej, pvalscorr = fdrcorrection(pvals, alpha=alpha_star, method='indep',\n",
    "                                       is_sorted=True)\n",
    "        ri = rej.sum()\n",
    "        if (not iter) or ri == ri_old:\n",
    "            break\n",
    "        elif ri < ri_old:\n",
    "            # prevent cycles and endless loops\n",
    "            raise RuntimeError(\" oops - should not be here\")\n",
    "        ri_old = ri\n",
    "\n",
    "    # make adjustment to pvalscorr to reflect estimated number of Non-Null cases\n",
    "    # decision is then pvalscorr < alpha  (or <=)\n",
    "    pvalscorr *= ntests0 * 1.0 /  ntests\n",
    "    if method == 'bky':\n",
    "        pvalscorr *= (1. + alpha)\n",
    "\n",
    "    if not is_sorted:\n",
    "        pvalscorr_ = np.empty_like(pvalscorr)\n",
    "        pvalscorr_[pvals_sortind] = pvalscorr\n",
    "        del pvalscorr\n",
    "        reject = np.empty_like(rej)\n",
    "        reject[pvals_sortind] = rej\n",
    "        return reject, pvalscorr_, ntests - ri, alpha_stages\n",
    "    else:\n",
    "        return rej, pvalscorr, ntests - ri, alpha_stages\n",
    "\n",
    "\n",
    "def local_fdr(zscores, null_proportion=1.0, null_pdf=None, deg=7,\n",
    "              nbins=30, alpha=0):\n",
    "    \"\"\"\n",
    "    Calculate local FDR values for a list of Z-scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    zscores : array_like\n",
    "        A vector of Z-scores\n",
    "    null_proportion : float\n",
    "        The assumed proportion of true null hypotheses\n",
    "    null_pdf : function mapping reals to positive reals\n",
    "        The density of null Z-scores; if None, use standard normal\n",
    "    deg : int\n",
    "        The maximum exponent in the polynomial expansion of the\n",
    "        density of non-null Z-scores\n",
    "    nbins : int\n",
    "        The number of bins for estimating the marginal density\n",
    "        of Z-scores.\n",
    "    alpha : float\n",
    "        Use Poisson ridge regression with parameter alpha to estimate\n",
    "        the density of non-null Z-scores.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fdr : array_like\n",
    "        A vector of FDR values\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    B Efron (2008).  Microarrays, Empirical Bayes, and the Two-Groups\n",
    "    Model.  Statistical Science 23:1, 1-22.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Basic use (the null Z-scores are taken to be standard normal):\n",
    "\n",
    "    >>> from statsmodels.stats.multitest import local_fdr\n",
    "    >>> import numpy as np\n",
    "    >>> zscores = np.random.randn(30)\n",
    "    >>> fdr = local_fdr(zscores)\n",
    "\n",
    "    Use a Gaussian null distribution estimated from the data:\n",
    "\n",
    "    >>> null = EmpiricalNull(zscores)\n",
    "    >>> fdr = local_fdr(zscores, null_pdf=null.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    from statsmodels.genmod.generalized_linear_model import GLM\n",
    "    from statsmodels.genmod.generalized_linear_model import families\n",
    "    from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "    # Bins for Poisson modeling of the marginal Z-score density\n",
    "    minz = min(zscores)\n",
    "    maxz = max(zscores)\n",
    "    bins = np.linspace(minz, maxz, nbins)\n",
    "\n",
    "    # Bin counts\n",
    "    zhist = np.histogram(zscores, bins)[0]\n",
    "\n",
    "    # Bin centers\n",
    "    zbins = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    # The design matrix at bin centers\n",
    "    dmat = np.vander(zbins, deg + 1)\n",
    "\n",
    "    # Rescale the design matrix\n",
    "    sd = dmat.std(0)\n",
    "    ii = sd >1e-8\n",
    "    dmat[:, ii] /= sd[ii]\n",
    "\n",
    "    start = OLS(np.log(1 + zhist), dmat).fit().params\n",
    "\n",
    "    # Poisson regression\n",
    "    if alpha > 0:\n",
    "        md = GLM(zhist, dmat, family=families.Poisson()).fit_regularized(L1_wt=0, alpha=alpha, start_params=start)\n",
    "    else:\n",
    "        md = GLM(zhist, dmat, family=families.Poisson()).fit(start_params=start)\n",
    "\n",
    "    # The design matrix for all Z-scores\n",
    "    dmat_full = np.vander(zscores, deg + 1)\n",
    "    dmat_full[:, ii] /= sd[ii]\n",
    "\n",
    "    # The height of the estimated marginal density of Z-scores,\n",
    "    # evaluated at every observed Z-score.\n",
    "    fz = md.predict(dmat_full) / (len(zscores) * (bins[1] - bins[0]))\n",
    "\n",
    "    # The null density.\n",
    "    if null_pdf is None:\n",
    "        f0 = np.exp(-0.5 * zscores**2) / np.sqrt(2 * np.pi)\n",
    "    else:\n",
    "        f0 = null_pdf(zscores)\n",
    "\n",
    "    # The local FDR values\n",
    "    fdr = null_proportion * f0 / fz\n",
    "\n",
    "    fdr = np.clip(fdr, 0, 1)\n",
    "\n",
    "    return fdr\n",
    "\n",
    "\n",
    "class NullDistribution(object):\n",
    "    \"\"\"\n",
    "    Estimate a Gaussian distribution for the null Z-scores.\n",
    "\n",
    "    The observed Z-scores consist of both null and non-null values.\n",
    "    The fitted distribution of null Z-scores is Gaussian, but may have\n",
    "    non-zero mean and/or non-unit scale.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    zscores : array_like\n",
    "        The observed Z-scores.\n",
    "    null_lb : float\n",
    "        Z-scores between `null_lb` and `null_ub` are all considered to be\n",
    "        true null hypotheses.\n",
    "    null_ub : float\n",
    "        See `null_lb`.\n",
    "    estimate_mean : bool\n",
    "        If True, estimate the mean of the distribution.  If False, the\n",
    "        mean is fixed at zero.\n",
    "    estimate_scale : bool\n",
    "        If True, estimate the scale of the distribution.  If False, the\n",
    "        scale parameter is fixed at 1.\n",
    "    estimate_null_proportion : bool\n",
    "        If True, estimate the proportion of true null hypotheses (i.e.\n",
    "        the proportion of z-scores with expected value zero).  If False,\n",
    "        this parameter is fixed at 1.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    mean : float\n",
    "        The estimated mean of the empirical null distribution\n",
    "    sd : float\n",
    "        The estimated standard deviation of the empirical null distribution\n",
    "    null_proportion : float\n",
    "        The estimated proportion of true null hypotheses among all hypotheses\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    B Efron (2008).  Microarrays, Empirical Bayes, and the Two-Groups\n",
    "    Model.  Statistical Science 23:1, 1-22.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    See also:\n",
    "\n",
    "    http://nipy.org/nipy/labs/enn.html#nipy.algorithms.statistics.empirical_pvalue.NormalEmpiricalNull.fdr\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, zscores, null_lb=-1, null_ub=1, estimate_mean=True,\n",
    "                 estimate_scale=True, estimate_null_proportion=False):\n",
    "\n",
    "        # Extract the null z-scores\n",
    "        ii = np.flatnonzero((zscores >= null_lb) & (zscores <= null_ub))\n",
    "        if len(ii) == 0:\n",
    "            raise RuntimeError(\"No Z-scores fall between null_lb and null_ub\")\n",
    "        zscores0 = zscores[ii]\n",
    "\n",
    "        # Number of Z-scores, and null Z-scores\n",
    "        n_zs, n_zs0 = len(zscores), len(zscores0)\n",
    "\n",
    "        # Unpack and transform the parameters to the natural scale, hold\n",
    "        # parameters fixed as specified.\n",
    "        def xform(params):\n",
    "\n",
    "            mean = 0.\n",
    "            sd = 1.\n",
    "            prob = 1.\n",
    "\n",
    "            ii = 0\n",
    "            if estimate_mean:\n",
    "                mean = params[ii]\n",
    "                ii += 1\n",
    "            if estimate_scale:\n",
    "                sd = np.exp(params[ii])\n",
    "                ii += 1\n",
    "            if estimate_null_proportion:\n",
    "                prob = 1 / (1 + np.exp(-params[ii]))\n",
    "\n",
    "            return mean, sd, prob\n",
    "\n",
    "\n",
    "        from scipy.stats.distributions import norm\n",
    "\n",
    "\n",
    "        def fun(params):\n",
    "            \"\"\"\n",
    "            Negative log-likelihood of z-scores.\n",
    "\n",
    "            The function has three arguments, packed into a vector:\n",
    "\n",
    "            mean : location parameter\n",
    "            logscale : log of the scale parameter\n",
    "            logitprop : logit of the proportion of true nulls\n",
    "\n",
    "            The implementation follows section 4 from Efron 2008.\n",
    "            \"\"\"\n",
    "\n",
    "            d, s, p = xform(params)\n",
    "\n",
    "            # Mass within the central region\n",
    "            central_mass = (norm.cdf((null_ub - d) / s) -\n",
    "                            norm.cdf((null_lb - d) / s))\n",
    "\n",
    "            # Probability that a Z-score is null and is in the central region\n",
    "            cp = p * central_mass\n",
    "\n",
    "            # Binomial term\n",
    "            rval = n_zs0 * np.log(cp) + (n_zs - n_zs0) * np.log(1 - cp)\n",
    "\n",
    "            # Truncated Gaussian term for null Z-scores\n",
    "            zv = (zscores0 - d) / s\n",
    "            rval += np.sum(-zv**2 / 2) - n_zs0 * np.log(s)\n",
    "            rval -= n_zs0 * np.log(central_mass)\n",
    "\n",
    "            return -rval\n",
    "\n",
    "\n",
    "        # Estimate the parameters\n",
    "        from scipy.optimize import minimize\n",
    "        # starting values are mean = 0, scale = 1, p0 ~ 1\n",
    "        mz = minimize(fun, np.r_[0., 0, 3], method=\"Nelder-Mead\")\n",
    "        mean, sd, prob = xform(mz['x'])\n",
    "\n",
    "        self.mean = mean\n",
    "        self.sd = sd\n",
    "        self.null_proportion = prob\n",
    "\n",
    "\n",
    "    # The fitted null density function\n",
    "    def pdf(self, zscores):\n",
    "        \"\"\"\n",
    "        Evaluates the fitted empirical null Z-score density.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        zscores : scalar or array_like\n",
    "            The point or points at which the density is to be\n",
    "            evaluated.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The empirical null Z-score density evaluated at the given\n",
    "        points.\n",
    "        \"\"\"\n",
    "\n",
    "        zval = (zscores - self.mean) / self.sd\n",
    "        return np.exp(-0.5*zval**2 - np.log(self.sd) - 0.5*np.log(2*np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SZ = Z_df.query('group == 0')\n",
    "HC = Z_df.query('group == 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass univariate two sample t-tests on deviation score maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBuQvJKqVz0p"
   },
   "outputs": [],
   "source": [
    "SZ_deviations = SZ.loc[:, SZ.columns.str.contains('Z_predict')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBuQvJKqVz0p"
   },
   "outputs": [],
   "source": [
    "HC_deviations = HC.loc[:, HC.columns.str.contains('Z_predict')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_cols = SZ_deviations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_hc_pvals_z = pd.DataFrame(columns={'roi','pval', 'tstat','fdr_pval'})\n",
    "for index, column in enumerate(z_cols):\n",
    "    test = ttest_ind(SZ_deviations[column], HC_deviations[column])\n",
    "    sz_hc_pvals_z.loc[index, 'pval'] = test.pvalue\n",
    "    sz_hc_pvals_z.loc[index, 'tstat'] = test.statistic\n",
    "    sz_hc_pvals_z.loc[index, 'roi'] = column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_hc_fdr_z = fdrcorrection(sz_hc_pvals_z['pval'], alpha=0.05, method='indep', is_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_hc_pvals_z['fdr_pval'] = sz_hc_fdr_z[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_hc_z_sig_diff = sz_hc_pvals_z.query('pval < 0.05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_hc_z_sig_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_hc_z_sig_diff.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass univariate two sample t-tests on deviation score maps and true cortical thickness data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZvu0iXlZg7P"
   },
   "outputs": [],
   "source": [
    "SZ_cortical_thickness = SZ.loc[:, SZ.columns.str.endswith('_thickness')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZvu0iXlZg7P"
   },
   "outputs": [],
   "source": [
    "HC_cortical_thickness = HC.loc[:, HC.columns.str.endswith('_thickness')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_cols = SZ_cortical_thickness.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_hc_pvals_ct = pd.DataFrame(columns={'roi','pval', 'tstat','fdr_pval'})\n",
    "for index, column in enumerate(ct_cols):\n",
    "    test = ttest_ind(SZ_cortical_thickness[column], HC_cortical_thickness[column])\n",
    "    sz_hc_pvals_ct.loc[index, 'pval'] = test.pvalue\n",
    "    sz_hc_pvals_ct.loc[index, 'tstat'] = test.statistic\n",
    "    sz_hc_pvals_ct.loc[index, 'roi'] = column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_hc_fdr_ct = fdrcorrection(sz_hc_pvals_ct['pval'], alpha=0.05, method='indep', is_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_hc_pvals_ct['fdr_pval'] = sz_hc_fdr_ct[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_hc_ct_sig_diff = sz_hc_pvals_ct.query('pval < 0.05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_hc_ct_sig_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz_hc_ct_sig_diff.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "4_post_hoc_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
