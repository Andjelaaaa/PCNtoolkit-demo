{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Predictive Clinical Neuroscience Toolkit](https://github.com/amarquand/PCNtoolkit) \n",
    "# Normative Modeling Tutorial Using Multi-Site Cortical Thickness Data\n",
    "# Part 2: Run the normative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created by [Saige Rutherford](https://twitter.com/being_saige) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../../data/NormModelSetup.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# re-run if you did not run part 1 notebook\n",
    "! git clone https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# re-run if you did not run part 1 notebook\n",
    "! cd PCNtoolkit-demo/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# re-run if you did not run part 1 notebook\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Run normative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import joypy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pcntoolkit.normative import estimate, evaluate\n",
    "from pcntoolkit.utils import create_bspline_basis, compute_MSLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this path to wherever your ROI_models folder is located (where you copied all of the covariate & response text files to in Part 1)\n",
    "data_dir = 'data/ROI_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all the ROIs you want to run a normative model for\n",
    "roi_ids = ['lh_MeanThickness_thickness',\n",
    "           'rh_MeanThickness_thickness',\n",
    "           'lh_bankssts_thickness',\n",
    "           'lh_caudalanteriorcingulate_thickness',\n",
    "           'lh_superiorfrontal_thickness',\n",
    "           'rh_superiorfrontal_thickness']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we split the data into train and test sets, we did not reset the index. This means that the row numbers in the train/test matrices are still the same as before splitting the data. We will need the test set row numbers of which subjects belong to which site in order to evaluate per site performance metrics, so we need to reset the row numbers in the train/test split matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col_names = ['age', 'sex', 'site_cam', 'site_hcp', 'site_ixi']\n",
    "X_train = pd.read_csv('data/covariate_files/cov_tr.txt', sep='\\t', header=None, names=x_col_names)\n",
    "X_test = pd.read_csv('data/covariate_files/cov_te.txt', sep='\\t', header=None, names=x_col_names)\n",
    "y_train = pd.read_csv('data/response_files/resp_tr.txt', sep='\\t', header=None)\n",
    "y_test = pd.read_csv('data/response_files/resp_te.txt', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract site indices\n",
    "\n",
    "Get site ids so that we can evaluate the test metrics independently for each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_idx = X_test.index[X_test['site_cam' ]== 1].to_list()\n",
    "hcp_idx = X_test.index[X_test['site_hcp'] == 1].to_list()\n",
    "ixi_idx = X_test.index[X_test['site_ixi'] == 1].to_list()\n",
    "\n",
    "# Save the site indices into a single list\n",
    "sites = [cam_idx, hcp_idx, ixi_idx]\n",
    "\n",
    "# Create a list with sites names to use in evaluating per-site metrics\n",
    "site_names = ['cam', 'hcp', 'ixi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis expansion\n",
    "\n",
    "Now, we set up a B-spline basis set that allows us to perform nonlinear regression using a linear model. This basis is deliberately chosen to not to be too flexible so that in can only model relatively slowly varying trends. To increase the flexibility of the model you can change the parameterisation (e.g. by adding knot points to the Bspline basis or increasing the order of the interpolating polynomial). \n",
    "\n",
    "Note that in the neuroimaging literature, it is more common to use a polynomial basis expansion for this. Piecewise polynomials like B-splines are superior because they do not introduce a global curvature. See the reference below for further information.\n",
    "\n",
    "[Primer on regression splines](https://cran.r-project.org/web/packages/crs/vignettes/spline_primer.pdf)\n",
    "\n",
    "[Reference for why polynomials are a bad idea](https://www.sciencedirect.com/science/article/abs/pii/S1053811910000832?via%3Dihub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating basis expansion for ROI: lh_MeanThickness_thickness\n",
      "Creating basis expansion for ROI: rh_MeanThickness_thickness\n",
      "Creating basis expansion for ROI: lh_bankssts_thickness\n",
      "Creating basis expansion for ROI: lh_caudalanteriorcingulate_thickness\n",
      "Creating basis expansion for ROI: lh_superiorfrontal_thickness\n",
      "Creating basis expansion for ROI: rh_superiorfrontal_thickness\n"
     ]
    }
   ],
   "source": [
    "# Create a cubic B-spline basis (used for regression)\n",
    "xmin = 10#16 # xmin & xmax are the boundaries for ages of participants in the dataset\n",
    "xmax = 95#90\n",
    "B = create_bspline_basis(xmin, xmax)\n",
    "\n",
    "# create the basis expansion for the covariates for each of the \n",
    "for roi in roi_ids: \n",
    "    print('Creating basis expansion for ROI:', roi)\n",
    "    roi_dir = os.path.join(data_dir, roi)\n",
    "    os.chdir(roi_dir)\n",
    "    \n",
    "    # create output dir \n",
    "    os.makedirs(os.path.join(roi_dir,'blr'), exist_ok=True)\n",
    "    \n",
    "    # load train & test covariate data matrices\n",
    "    X_tr = np.loadtxt(os.path.join(roi_dir, 'cov_tr.txt'))\n",
    "    X_te = np.loadtxt(os.path.join(roi_dir, 'cov_te.txt'))\n",
    "\n",
    "    # add intercept column \n",
    "    X_tr = np.concatenate((X_tr, np.ones((X_tr.shape[0],1))), axis=1)\n",
    "    X_te = np.concatenate((X_te, np.ones((X_te.shape[0],1))), axis=1)\n",
    "    np.savetxt(os.path.join(roi_dir, 'cov_int_tr.txt'), X_tr)\n",
    "    np.savetxt(os.path.join(roi_dir, 'cov_int_te.txt'), X_te)\n",
    "    \n",
    "    # create Bspline basis set \n",
    "    Phi = np.array([B(i) for i in X_tr[:,0]])\n",
    "    Phis = np.array([B(i) for i in X_te[:,0]])\n",
    "    X_tr = np.concatenate((X_tr, Phi), axis=1)\n",
    "    X_te = np.concatenate((X_te, Phis), axis=1)\n",
    "    np.savetxt(os.path.join(roi_dir, 'cov_bspline_tr.txt'), X_tr)\n",
    "    np.savetxt(os.path.join(roi_dir, 'cov_bspline_te.txt'), X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare output structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas dataframes with header names to save out the overall and per-site model evaluation metrics\n",
    "blr_metrics = pd.DataFrame(columns = ['ROI', 'MSLL', 'EV', 'SMSE', 'RMSE', 'Rho'])\n",
    "blr_site_metrics = pd.DataFrame(columns = ['ROI', 'site', 'y_mean', 'y_var', 'yhat_mean', 'yhat_var', 'MSLL', 'EV', 'SMSE', 'RMSE', 'Rho'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the normative models\n",
    "\n",
    "In this step, we estimate the normative models one at a time. In principle we could also do this on the whole data matrix at once (e.g. with the response variables stored in a n_subjects x n_brain_measures numpy array). However, doing it this way gives us some extra flexibility in that it does not require that the subjects are exactly the same for each of the brain measures. \n",
    "\n",
    "This code fragment will loop through each region of interest in the roi_ids list (set a few code blocks above) using Bayesian linear regression and evaluate the model on the independent test set. It will then compute error metrics such as the explained variance, mean standardized log loss and Pearson correlation between true and predicted test responses separately for each scanning site. \n",
    "\n",
    "We supply the estimate function with a few specific arguments that are worthy of commenting on: \n",
    "* alg = 'blr' : specifies we should use Bayesian linear regression\n",
    "* optimizer = 'powell' : use Powell's derivative-free optimization method (faster in this case than L-BFGS)\n",
    "* savemodel = False : do not write out the final estimated model to disk\n",
    "* saveoutput = False : return the outputs directly rather than writing them to disk\n",
    "* standardize = False : Do not standardize the covariates or response variables\n",
    "\n",
    "One important consideration is whether or not to standardize. Whilst this generally only has a minor effect on the final model accuracy, it has implications for the interpretation of models and how they are configured. If the covariates and responses are both standardized, the model will return standardized coefficients. If (as in this case) the response variables are not standardized, then the scaling both covariates and responses will be reflected in the estimated coefficients. Also, under the linear modelling approach employed here, if the coefficients are unstandardized and do not have a zero mean, it is necessary to add an intercept column to the design matrix. This is done in the code block above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through ROIs\n",
    "for roi in roi_ids: \n",
    "    print('Running ROI:', roi)\n",
    "    roi_dir = os.path.join(data_dir, roi)\n",
    "    os.chdir(roi_dir)\n",
    "     \n",
    "    # configure the covariates to use. Change *_bspline_* to *_int_* to \n",
    "    cov_file_tr = os.path.join(roi_dir, 'cov_bspline_tr.txt')\n",
    "    cov_file_te = os.path.join(roi_dir, 'cov_bspline_te.txt')\n",
    "    \n",
    "    # load train & test response files\n",
    "    resp_file_tr = os.path.join(roi_dir, 'resp_tr.txt')\n",
    "    resp_file_te = os.path.join(roi_dir, 'resp_te.txt') \n",
    "    \n",
    "    # run a basic model\n",
    "    yhat_te, s2_te, nm, Z, metrics_te = estimate(cov_file_tr, \n",
    "                                                 resp_file_tr, \n",
    "                                                 testresp=resp_file_te, \n",
    "                                                 testcov=cov_file_te, \n",
    "                                                 alg = 'blr', \n",
    "                                                 optimizer = 'powell', \n",
    "                                                 savemodel = False, \n",
    "                                                 saveoutput = False,\n",
    "                                                 standardize = False)\n",
    "    # display and save metrics\n",
    "    print('EV=', metrics_te['EXPV'][0])\n",
    "    print('RHO=', metrics_te['Rho'][0])\n",
    "    print('MSLL=', metrics_te['MSLL'][0])\n",
    "    blr_metrics.loc[len(blr_metrics)] = [roi, metrics_te['MSLL'][0], metrics_te['EXPV'][0], metrics_te['SMSE'][0], \n",
    "                                         metrics_te['RMSE'][0], metrics_te['Rho'][0]]\n",
    "    \n",
    "    # Compute metrics per site in test set, save to pandas df\n",
    "    # load true test data\n",
    "    X_te = np.loadtxt(cov_file_te)\n",
    "    y_te = np.loadtxt(resp_file_te)\n",
    "    y_te = y_te[:, np.newaxis] # make sure it is a 2-d array\n",
    "    \n",
    "    # load training data (required to compute the MSLL)\n",
    "    y_tr = np.loadtxt(resp_file_tr)\n",
    "    y_tr = y_tr[:, np.newaxis]\n",
    "    \n",
    "    for num, site in enumerate(sites):     \n",
    "        y_mean_te_site = np.array([[np.mean(y_te[site])]])\n",
    "        y_var_te_site = np.array([[np.var(y_te[site])]])\n",
    "        yhat_mean_te_site = np.array([[np.mean(yhat_te[site])]])\n",
    "        yhat_var_te_site = np.array([[np.var(yhat_te[site])]])\n",
    "        \n",
    "        metrics_te_site = evaluate(y_te[site], yhat_te[site], s2_te[site], y_mean_te_site, y_var_te_site)\n",
    "        \n",
    "        site_name = site_names[num]\n",
    "        blr_site_metrics.loc[len(blr_site_metrics)] = [roi, site_names[num],\n",
    "                                                       y_mean_te_site[0],\n",
    "                                                       y_var_te_site[0],\n",
    "                                                       yhat_mean_te_site[0],\n",
    "                                                       yhat_var_te_site[0],\n",
    "                                                       metrics_te_site['MSLL'][0],\n",
    "                                                       metrics_te_site['EXPV'][0],\n",
    "                                                       metrics_te_site['SMSE'][0],\n",
    "                                                       metrics_te_site['RMSE'][0],\n",
    "                                                       metrics_te_site['Rho'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save per site test set metrics variable to CSV file\n",
    "blr_site_metrics.to_csv('blr_site_metrics.csv', index=False, index_label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save overall test set metrics to CSV file\n",
    "blr_metrics.to_csv('blr_metrics.csv', index=False, index_label=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Interpreting model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output evaluation metrics definitions: \n",
    "* yhat - predictive mean\n",
    "* ys2 - predictive variance\n",
    "* nm - normative model\n",
    "* Z - deviance scores\n",
    "* Rho - Pearson correlation between true and predicted responses\n",
    "* pRho - parametric p-value for this correlation\n",
    "* RMSE - root mean squared error between true/predicted responses\n",
    "* SMSE - standardised mean squared error\n",
    "* EV - explained variance\n",
    "* MSLL - mean standardized log loss\n",
    "    * See page 23 in http://www.gaussianprocess.org/gpml/chapters/RW2.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}