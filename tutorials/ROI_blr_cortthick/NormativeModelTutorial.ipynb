{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Predictive Clinical Neuroscience Toolkit](https://github.com/amarquand/PCNtoolkit) \n",
    "# Normative Modeling Tutorial Using Multi-Site Cortical Thickness Data\n",
    "\n",
    "This notebook will prepare the data for normative modelling (assembling data matrices from different datasets, preparing the covariates etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created by [Saige Rutherford](https://twitter.com/being_saige) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../../data/NormModelSetup.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Install necessary libraries & grab data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this path to the git cloned PCNtoolkit-demo repository --> Uncomment whichever line you need for either running on your own computer or on Google Colab.\n",
    "#os.chdir('/Users/saigerutherford/repos/PCNtoolkit-demo/') # if running on your own computer, use this line (but obvi change the path)\n",
    "#os.chdir('PCNtoolkit-demo/') # if running on Google Colab, use this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare covariate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we will use data from the [Human Connectome Project Young Adult study](https://www.humanconnectome.org/study/hcp-young-adult), [CAMCAN](https://www.cam-can.org/), and [IXI](https://brain-development.org/ixi-dataset/) to create a multi-site dataset. \n",
    "\n",
    "Our first step is to prepare and combine the covariate (age & sex) data from each site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joypy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pcntoolkit.normative import estimate, evaluate\n",
    "from pcntoolkit.utils import create_bspline_basis, compute_MSLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcp = pd.read_csv('data/HCP1200_age_gender.csv')\n",
    "cam = pd.read_csv('data/cam_age_gender.csv')\n",
    "ixi = pd.read_csv('data/IXI_age_gender.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_hcp = pd.merge(hcp, cam, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = pd.merge(cam_hcp, ixi, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5, style='darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(cov, x=\"age\", hue=\"site\", multiple=\"stack\", height=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.groupby(['site']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Prepare brain data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will format and combine the MRI data. We are using cortical thickness maps that are created by running recon-all from Freesurfer 6. We need to merge together the left and right hemisphere text files for each site, and then combine the different sites into a single dataframe. We reduce the dimensionality of our data by using ROIs from the Desikan-Killiany atlas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some psuedo-code (run from a terminal in the folder that has all subject's recon-all output folders) that was used to extract these ROIs:\n",
    "\n",
    "```export SUBJECTS_DIR=/path/to/study/freesurfer_data/```\n",
    "\n",
    "```aparcstats2table --subject sub-* --hemi lh --meas thickness --tablefile HCP1200_aparc_lh_thickness.txt```\n",
    "\n",
    "```aparcstats2table --subject sub-* --hemi rh --meas thickness --tablefile HCP1200_aparc_rh_thickness.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = pd.read_csv('data/CAMCAN_aparc_thickness.csv')\n",
    "hcpya = pd.read_csv('data/HCP1200_aparc_thickness.csv')\n",
    "ixi = pd.read_csv('data/IXI_aparc_thickness.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcpya_cam = pd.merge(hcpya, cam, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_all = pd.merge(ixi, hcpya_cam, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to include the [Euler number](https://mathworld.wolfram.com/EulerCharacteristic.html) as a covariate. So we extracted the euler number from each subject's recon-all output folder into a text file and we now need to format and combine these into our brain dataframe. \n",
    "\n",
    "Below is psuedo code for how we extracted the euler number from the recon-all.log for each subject. Run this from the terminal in the folder where your subjects recon-all output folders are located. This assumes that all of your subject IDs start with \"sub-\" prefix.\n",
    "\n",
    "```for i in sub-*; do if [[ -e ${i}/scripts/recon-all.log ]]; then cat ${i}/scripts/recon-all.log | grep -A 1 \"Computing euler\" > temp_log; lh_en=`cat temp_log | head -2 | tail -1 | awk -F '=' '{print $2}' | awk -F ',' '{print $1}'`; rh_en=`cat temp_log | head -2 | tail -1 | awk -F '=' '{print $3}'`; echo \"${i}, ${lh_en}, ${rh_en}\" >> euler.csv; echo ${i}; fi; done```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcp_euler = pd.read_csv('data/hcp-ya_euler.csv')\n",
    "cam_euler = pd.read_csv('data/cam_euler.csv')\n",
    "ixi_euler = pd.read_csv('data/ixi_euler.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcp_euler['site'] = 'hcp'\n",
    "cam_euler['site'] = 'cam'\n",
    "ixi_euler['site'] = 'ixi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcp_euler.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "cam_euler.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "ixi_euler.replace(r'^\\s*$', np.nan, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcp_euler.dropna(inplace=True)\n",
    "cam_euler.dropna(inplace=True)\n",
    "ixi_euler.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcp_euler['rh_euler'] = hcp_euler['rh_euler'].astype(int)\n",
    "hcp_euler['lh_euler'] = hcp_euler['lh_euler'].astype(int)\n",
    "cam_euler['rh_euler'] = cam_euler['rh_euler'].astype(int)\n",
    "cam_euler['lh_euler'] = cam_euler['lh_euler'].astype(int)\n",
    "ixi_euler['rh_euler'] = ixi_euler['rh_euler'].astype(int)\n",
    "ixi_euler['lh_euler'] = ixi_euler['lh_euler'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcp_cam_euler = pd.merge(hcp_euler, cam_euler, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_euler = pd.merge(ixi_euler, hcp_cam_euler, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to center the euler number for each site. The euler number is very site-specific so in order to use the same exclusion threshold across sites we need to center the site by subtracting the site median from all subjects at a site. Then we will take the square root and multiply by negative one and exclude any subjects with a square root above 10. This choice of threshold is fairly random. If possible all of your data should be visually inspected to verify that the data inclusion is not too strict or too lenient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_euler['avg_euler'] = df_euler[['lh_euler','rh_euler']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_euler.groupby(by='site').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_euler['site_median'] = df_euler['site']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_euler['site_median'] = df_euler['site_median'].replace({'hcp':-43,'cam':-61,'ixi':-56})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_euler['avg_euler_centered'] = df_euler['avg_euler'] - df_euler['site_median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_euler['avg_euler_centered_neg'] = df_euler['avg_euler_centered']*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_euler['avg_euler_centered_neg_sqrt'] = np.sqrt(np.absolute(df_euler['avg_euler_centered_neg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "#create a color gradent function to be used in the colormap parameter\n",
    "def color_gradient(x=0.0, start=(0, 0, 0), stop=(1, 1, 1)):\n",
    "    r = np.interp(x, [0, 1], [start[0], stop[0]])\n",
    "    g = np.interp(x, [0, 1], [start[1], stop[1]])\n",
    "    b = np.interp(x, [0, 1], [start[2], stop[2]])\n",
    "    return r, g, b#show the table\n",
    "#plot the figure\n",
    "plt.figure(dpi=380)\n",
    "fig, axes = joypy.joyplot(df_euler, column=['avg_euler_centered_neg_sqrt'], overlap=2.5, by=\"site\", ylim='own', fill=True, figsize=(6,6)\n",
    "                          , legend=False, xlabels=True, ylabels=True, colormap=lambda x: color_gradient(x, start=(.08, .45, .8),stop=(.8, .34, .44))\n",
    "                          , alpha=0.6, linewidth=.5, linecolor='w', fade=True)\n",
    "plt.title('sqrt(-Euler Number), median centered', fontsize=18, color='black', alpha=1)\n",
    "plt.xlabel('sqrt(-Euler number)', fontsize=14, color='black', alpha=1)\n",
    "plt.ylabel('Site', fontsize=14, color='black', alpha=1)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = pd.merge(df_euler, brain_all, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_good = brain.query('avg_euler_centered_neg_sqrt < 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brain_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lose 63 subjects because they have a large euler number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Combine covariate & cortical thickness dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the normative modeling code needs the covariate and features (cortical thickness) in separate text files, we first need to merge them together to make sure that we have the same subjects in each file and that the rows (representing subjects) align. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to use how=\"inner\" so that we only include subjects that have data in both the covariate and the cortical thickness files \n",
    "all_data = pd.merge(brain_good, cov, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Format dataframes to run normative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any subjects that have NaN variables in any of the columns\n",
    "all_data.dropna(subset=['lh_bankssts_thickness',\n",
    "       'lh_caudalanteriorcingulate_thickness',\n",
    "       'lh_caudalmiddlefrontal_thickness', 'lh_cuneus_thickness',\n",
    "       'lh_entorhinal_thickness', 'lh_fusiform_thickness',\n",
    "       'lh_inferiorparietal_thickness', 'lh_inferiortemporal_thickness',\n",
    "       'lh_isthmuscingulate_thickness', 'lh_lateraloccipital_thickness',\n",
    "       'lh_lateralorbitofrontal_thickness', 'lh_lingual_thickness',\n",
    "       'lh_medialorbitofrontal_thickness', 'lh_middletemporal_thickness',\n",
    "       'lh_parahippocampal_thickness', 'lh_paracentral_thickness',\n",
    "       'lh_parsopercularis_thickness', 'lh_parsorbitalis_thickness',\n",
    "       'lh_parstriangularis_thickness', 'lh_pericalcarine_thickness',\n",
    "       'lh_postcentral_thickness', 'lh_posteriorcingulate_thickness',\n",
    "       'lh_precentral_thickness', 'lh_precuneus_thickness',\n",
    "       'lh_rostralanteriorcingulate_thickness',\n",
    "       'lh_rostralmiddlefrontal_thickness', 'lh_superiorfrontal_thickness',\n",
    "       'lh_superiorparietal_thickness', 'lh_superiortemporal_thickness',\n",
    "       'lh_supramarginal_thickness', 'lh_frontalpole_thickness',\n",
    "       'lh_temporalpole_thickness', 'lh_transversetemporal_thickness',\n",
    "       'lh_insula_thickness', 'lh_MeanThickness_thickness',\n",
    "       'rh_bankssts_thickness', 'rh_caudalanteriorcingulate_thickness',\n",
    "       'rh_caudalmiddlefrontal_thickness', 'rh_cuneus_thickness',\n",
    "       'rh_entorhinal_thickness', 'rh_fusiform_thickness',\n",
    "       'rh_inferiorparietal_thickness', 'rh_inferiortemporal_thickness',\n",
    "       'rh_isthmuscingulate_thickness', 'rh_lateraloccipital_thickness',\n",
    "       'rh_lateralorbitofrontal_thickness', 'rh_lingual_thickness',\n",
    "       'rh_medialorbitofrontal_thickness', 'rh_middletemporal_thickness',\n",
    "       'rh_parahippocampal_thickness', 'rh_paracentral_thickness',\n",
    "       'rh_parsopercularis_thickness', 'rh_parsorbitalis_thickness',\n",
    "       'rh_parstriangularis_thickness', 'rh_pericalcarine_thickness',\n",
    "       'rh_postcentral_thickness', 'rh_posteriorcingulate_thickness',\n",
    "       'rh_precentral_thickness', 'rh_precuneus_thickness',\n",
    "       'rh_rostralanteriorcingulate_thickness',\n",
    "       'rh_rostralmiddlefrontal_thickness', 'rh_superiorfrontal_thickness',\n",
    "       'rh_superiorparietal_thickness', 'rh_superiortemporal_thickness',\n",
    "       'rh_supramarginal_thickness', 'rh_frontalpole_thickness',\n",
    "       'rh_temporalpole_thickness', 'rh_transversetemporal_thickness',\n",
    "       'rh_insula_thickness', 'rh_MeanThickness_thickness','age','sex'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the covariate & features into their own dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_features = all_data[['lh_bankssts_thickness',\n",
    "       'lh_caudalanteriorcingulate_thickness',\n",
    "       'lh_caudalmiddlefrontal_thickness', 'lh_cuneus_thickness',\n",
    "       'lh_entorhinal_thickness', 'lh_fusiform_thickness',\n",
    "       'lh_inferiorparietal_thickness', 'lh_inferiortemporal_thickness',\n",
    "       'lh_isthmuscingulate_thickness', 'lh_lateraloccipital_thickness',\n",
    "       'lh_lateralorbitofrontal_thickness', 'lh_lingual_thickness',\n",
    "       'lh_medialorbitofrontal_thickness', 'lh_middletemporal_thickness',\n",
    "       'lh_parahippocampal_thickness', 'lh_paracentral_thickness',\n",
    "       'lh_parsopercularis_thickness', 'lh_parsorbitalis_thickness',\n",
    "       'lh_parstriangularis_thickness', 'lh_pericalcarine_thickness',\n",
    "       'lh_postcentral_thickness', 'lh_posteriorcingulate_thickness',\n",
    "       'lh_precentral_thickness', 'lh_precuneus_thickness',\n",
    "       'lh_rostralanteriorcingulate_thickness',\n",
    "       'lh_rostralmiddlefrontal_thickness', 'lh_superiorfrontal_thickness',\n",
    "       'lh_superiorparietal_thickness', 'lh_superiortemporal_thickness',\n",
    "       'lh_supramarginal_thickness', 'lh_frontalpole_thickness',\n",
    "       'lh_temporalpole_thickness', 'lh_transversetemporal_thickness',\n",
    "       'lh_insula_thickness', 'lh_MeanThickness_thickness',\n",
    "       'rh_bankssts_thickness', 'rh_caudalanteriorcingulate_thickness',\n",
    "       'rh_caudalmiddlefrontal_thickness', 'rh_cuneus_thickness',\n",
    "       'rh_entorhinal_thickness', 'rh_fusiform_thickness',\n",
    "       'rh_inferiorparietal_thickness', 'rh_inferiortemporal_thickness',\n",
    "       'rh_isthmuscingulate_thickness', 'rh_lateraloccipital_thickness',\n",
    "       'rh_lateralorbitofrontal_thickness', 'rh_lingual_thickness',\n",
    "       'rh_medialorbitofrontal_thickness', 'rh_middletemporal_thickness',\n",
    "       'rh_parahippocampal_thickness', 'rh_paracentral_thickness',\n",
    "       'rh_parsopercularis_thickness', 'rh_parsorbitalis_thickness',\n",
    "       'rh_parstriangularis_thickness', 'rh_pericalcarine_thickness',\n",
    "       'rh_postcentral_thickness', 'rh_posteriorcingulate_thickness',\n",
    "       'rh_precentral_thickness', 'rh_precuneus_thickness',\n",
    "       'rh_rostralanteriorcingulate_thickness',\n",
    "       'rh_rostralmiddlefrontal_thickness', 'rh_superiorfrontal_thickness',\n",
    "       'rh_superiorparietal_thickness', 'rh_superiortemporal_thickness',\n",
    "       'rh_supramarginal_thickness', 'rh_frontalpole_thickness',\n",
    "       'rh_temporalpole_thickness', 'rh_transversetemporal_thickness',\n",
    "       'rh_insula_thickness', 'rh_MeanThickness_thickness']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_covariates = all_data[['age','sex','site']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the sites are coded in a single column using a string. We need to instead dummy encode the site variable so that there is a column for each site and the columns contain binary variables (0/1). Luckily pandas has a nice built in function, ```pd.get_dummies``` to help us format the site column this way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_covariates = pd.get_dummies(all_data_covariates, columns=['site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Average_Thickness'] = all_data[['lh_MeanThickness_thickness','rh_MeanThickness_thickness']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a sneak peak to see if there are any super obvious site effects. If there were, we would see a large separation in the fitted regression line for each site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\",font_scale=1.5)\n",
    "c = sns.lmplot(data=all_data, x=\"age\", y=\"Average_Thickness\", hue=\"site\", height=6)\n",
    "plt.ylim(1.5, 3.25)\n",
    "plt.xlim(15, 95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 80% of the data for training and 20% for testing. We stratify our train/test split using the site variable to make sure that the train/test sets both contain data from all sites. The model wouldn't learn the site effects if all of the data from one site was only in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_data_covariates, all_data_features, stratify=all_data['site'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that your train & test arrays are the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_cov_size = X_train.shape\n",
    "tr_resp_size = y_train.shape\n",
    "te_cov_size = X_test.shape\n",
    "te_resp_size = y_test.shape\n",
    "print(\"Train covariate size is: \", tr_cov_size)\n",
    "print(\"Test covariate size is: \", te_cov_size)\n",
    "print(\"Train response size is: \", tr_resp_size)\n",
    "print(\"Test response size is: \", te_resp_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save out each ROI to its own file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup the normative model so that for each Y (brain region) we fit a separate model. While the estimate function in the pcntoolkit can handle having all of the Y's in a single text file, for this tutorial we are going to organize our Y's so that they are each in their own text file and directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/saigerutherford/repos/PCNToolkit-demo/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in y_train.columns:\n",
    "    y_train[c].to_csv('resp_tr_' + c + '.txt', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('cov_tr.txt', sep = '\\t', header=False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv('resp_tr.txt', sep = '\\t', header=False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in y_test.columns:\n",
    "    y_test[c].to_csv('resp_te_' + c + '.txt', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('cov_te.txt', sep = '\\t', header=False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.to_csv('resp_te.txt', sep = '\\t', header=False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! if [[ ! -e data/ROI_models/ ]]; then mkdir data/ROI_models; fi\n",
    "! if [[ ! -e data/covariate_files/ ]]; then mkdir data/covariate_files; fi\n",
    "! if [[ ! -e data/response_files/ ]]; then mkdir data/response_files; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! for i in `cat data/roi_dir_names`; do cd data/ROI_models; mkdir ${i}; cd ../../; cp resp_tr_${i}.txt data/ROI_models/${i}/resp_tr.txt; cp resp_te_${i}.txt data/ROI_models/${i}/resp_te.txt; cp cov_tr.txt data/ROI_models/${i}/cov_tr.txt; cp cov_te.txt data/ROI_models/${i}/cov_te.txt; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv resp_*.txt data/response_files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! mv cov_t*.txt data/covariate_files/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Run normative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this path to wherever your ROI_models folder is located (where you copied all of the covariate & response text files to in Step 4)\n",
    "data_dir = '/Users/saigerutherford/repos/PCNToolkit-demo/data/ROI_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all the ROIs you want to run a normative model for\n",
    "roi_ids = ['lh_MeanThickness_thickness',\n",
    "           'rh_MeanThickness_thickness',\n",
    "           'lh_bankssts_thickness',\n",
    "           'lh_caudalanteriorcingulate_thickness',\n",
    "           'lh_superiorfrontal_thickness',\n",
    "           'rh_superiorfrontal_thickness']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we split the data into train and test sets, we did not reset the index. This means that the row numbers in the train/test matrices are still the same as before splitting the data. We will need the test set row numbers of which subjects belong to which site in order to evaluate per site performance metrics, so we need to reset the row numbers in the train/test split matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col_names = ['age', 'sex', 'site_cam', 'site_hcp', 'site_ixi']\n",
    "X_train = pd.read_csv('data/covariate_files/cov_tr.txt', sep='\\t', header=None, names=x_col_names)\n",
    "X_test = pd.read_csv('data/covariate_files/cov_te.txt', sep='\\t', header=None, names=x_col_names)\n",
    "y_train = pd.read_csv('data/response_files/resp_tr.txt', sep='\\t', header=None)\n",
    "y_test = pd.read_csv('data/response_files/resp_te.txt', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract site indices\n",
    "\n",
    "Get site ids so that we can evaluate the test metrics independently for each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_idx = X_test.index[X_test['site_cam' ]== 1].to_list()\n",
    "hcp_idx = X_test.index[X_test['site_hcp'] == 1].to_list()\n",
    "ixi_idx = X_test.index[X_test['site_ixi'] == 1].to_list()\n",
    "\n",
    "# Save the site indices into a single list\n",
    "sites = [cam_idx, hcp_idx, ixi_idx]\n",
    "\n",
    "# Create a list with sites names to use in evaluating per-site metrics\n",
    "site_names = ['cam', 'hcp', 'ixi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis expansion\n",
    "\n",
    "Now, we set up a B-spline basis set that allows us to perform nonlinear regression using a linear model. This basis is deliberately chosen to not to be too flexible so that in can only model relatively slowly varying trends. To increase the flexibility of the model you can change the parameterisation (e.g. by adding knot points to the Bspline basis or increasing the order of the interpolating polynomial). \n",
    "\n",
    "Note that in the neuroimaging literature, it is more common to use a polynomial basis expansion for this. Piecewise polynomials like B-splines are superior because they do not introduce a global curvature. See the reference below for further information.\n",
    "\n",
    "[Primer on regression splines](https://cran.r-project.org/web/packages/crs/vignettes/spline_primer.pdf)\n",
    "\n",
    "[Reference for why polynomials are a bad idea](https://www.sciencedirect.com/science/article/abs/pii/S1053811910000832?via%3Dihub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cubic B-spline basis (used for regression)\n",
    "xmin = 10#16 # xmin & xmax are the boundaries for ages of participants in the dataset\n",
    "xmax = 95#90\n",
    "B = create_bspline_basis(xmin, xmax)\n",
    "\n",
    "# create the basis expansion for the covariates for each of the \n",
    "for roi in roi_ids: \n",
    "    print('Creating basis expansion for ROI:', roi)\n",
    "    roi_dir = os.path.join(data_dir, roi)\n",
    "    os.chdir(roi_dir)\n",
    "    \n",
    "    # create output dir \n",
    "    os.makedirs(os.path.join(roi_dir,'blr'), exist_ok=True)\n",
    "    \n",
    "    # load train & test covariate data matrices\n",
    "    X_tr = np.loadtxt(os.path.join(roi_dir, 'cov_tr.txt'))\n",
    "    X_te = np.loadtxt(os.path.join(roi_dir, 'cov_te.txt'))\n",
    "\n",
    "    # add intercept column \n",
    "    X_tr = np.concatenate((X_tr, np.ones((X_tr.shape[0],1))), axis=1)\n",
    "    X_te = np.concatenate((X_te, np.ones((X_te.shape[0],1))), axis=1)\n",
    "    np.savetxt(os.path.join(roi_dir, 'cov_int_tr.txt'), X_tr)\n",
    "    np.savetxt(os.path.join(roi_dir, 'cov_int_te.txt'), X_te)\n",
    "    \n",
    "    # create Bspline basis set \n",
    "    Phi = np.array([B(i) for i in X_tr[:,0]])\n",
    "    Phis = np.array([B(i) for i in X_te[:,0]])\n",
    "    X_tr = np.concatenate((X_tr, Phi), axis=1)\n",
    "    X_te = np.concatenate((X_te, Phis), axis=1)\n",
    "    np.savetxt(os.path.join(roi_dir, 'cov_bspline_tr.txt'), X_tr)\n",
    "    np.savetxt(os.path.join(roi_dir, 'cov_bspline_te.txt'), X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare output structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas dataframes with header names to save out the overall and per-site model evaluation metrics\n",
    "blr_metrics = pd.DataFrame(columns = ['ROI', 'MSLL', 'EV', 'SMSE', 'RMSE', 'Rho'])\n",
    "blr_site_metrics = pd.DataFrame(columns = ['ROI', 'site', 'y_mean', 'y_var', 'yhat_mean', 'yhat_var', 'MSLL', 'EV', 'SMSE', 'RMSE', 'Rho'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the normative models\n",
    "\n",
    "In this step, we estimate the normative models one at a time. In principle we could also do this on the whole data matrix at once (e.g. with the response variables stored in a n_subjects x n_brain_measures numpy array). However, doing it this way gives us some extra flexibility in that it does not require that the subjects are exactly the same for each of the brain measures. \n",
    "\n",
    "This code fragment will loop through each region of interest in the roi_ids list (set a few code blocks above) using Bayesian linear regression and evaluate the model on the independent test set. It will then compute error metrics such as the explained variance, mean standardized log loss and Pearson correlation between true and predicted test responses separately for each scanning site. \n",
    "\n",
    "We supply the estimate function with a few specific arguments that are worthy of commenting on: \n",
    "* alg = 'blr' : specifies we should use Bayesian linear regression\n",
    "* optimizer = 'powell' : use Powell's derivative-free optimization method (faster in this case than L-BFGS)\n",
    "* savemodel = False : do not write out the final estimated model to disk\n",
    "* saveoutput = False : return the outputs directly rather than writing them to disk\n",
    "* standardize = False : Do not standardize the covariates or response variables\n",
    "\n",
    "One important consideration is whether or not to standardize. Whilst this generally only has a minor effect on the final model accuracy, it has implications for the interpretation of models and how they are configured. If the covariates and responses are both standardized, the model will return standardized coefficients. If (as in this case) the response variables are not standardized, then the scaling both covariates and responses will be reflected in the estimated coefficients. Also, under the linear modelling approach employed here, if the coefficients are unstandardized and do not have a zero mean, it is necessary to add an intercept column to the design matrix. This is done in the code block above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through ROIs\n",
    "for roi in roi_ids: \n",
    "    print('Running ROI:', roi)\n",
    "    roi_dir = os.path.join(data_dir, roi)\n",
    "    os.chdir(roi_dir)\n",
    "     \n",
    "    # configure the covariates to use. Change *_bspline_* to *_int_* to \n",
    "    cov_file_tr = os.path.join(roi_dir, 'cov_bspline_tr.txt')\n",
    "    cov_file_te = os.path.join(roi_dir, 'cov_bspline_te.txt')\n",
    "    \n",
    "    # load train & test response files\n",
    "    resp_file_tr = os.path.join(roi_dir, 'resp_tr.txt')\n",
    "    resp_file_te = os.path.join(roi_dir, 'resp_te.txt') \n",
    "    \n",
    "    # run a basic model\n",
    "    yhat_te, s2_te, nm, Z, metrics_te = estimate(cov_file_tr, \n",
    "                                                 resp_file_tr, \n",
    "                                                 testresp=resp_file_te, \n",
    "                                                 testcov=cov_file_te, \n",
    "                                                 alg = 'blr', \n",
    "                                                 optimizer = 'powell', \n",
    "                                                 savemodel = False, \n",
    "                                                 saveoutput = False,\n",
    "                                                 standardize = False)\n",
    "    # display and save metrics\n",
    "    print('EV=', metrics_te['EXPV'][0])\n",
    "    print('RHO=', metrics_te['Rho'][0])\n",
    "    print('MSLL=', metrics_te['MSLL'][0])\n",
    "    blr_metrics.loc[len(blr_metrics)] = [roi, metrics_te['MSLL'][0], metrics_te['EXPV'][0], metrics_te['SMSE'][0], \n",
    "                                         metrics_te['RMSE'][0], metrics_te['Rho'][0]]\n",
    "    \n",
    "    # Compute metrics per site in test set, save to pandas df\n",
    "    # load true test data\n",
    "    X_te = np.loadtxt(cov_file_te)\n",
    "    y_te = np.loadtxt(resp_file_te)\n",
    "    y_te = y_te[:, np.newaxis] # make sure it is a 2-d array\n",
    "    \n",
    "    # load training data (required to compute the MSLL)\n",
    "    y_tr = np.loadtxt(resp_file_tr)\n",
    "    y_tr = y_tr[:, np.newaxis]\n",
    "    \n",
    "    for num, site in enumerate(sites):     \n",
    "        y_mean_te_site = np.array([[np.mean(y_te[site])]])\n",
    "        y_var_te_site = np.array([[np.var(y_te[site])]])\n",
    "        yhat_mean_te_site = np.array([[np.mean(yhat_te[site])]])\n",
    "        yhat_var_te_site = np.array([[np.var(yhat_te[site])]])\n",
    "        \n",
    "        metrics_te_site = evaluate(y_te[site], yhat_te[site], s2_te[site], y_mean_te_site, y_var_te_site)\n",
    "        \n",
    "        site_name = site_names[num]\n",
    "        blr_site_metrics.loc[len(blr_site_metrics)] = [roi, site_names[num],\n",
    "                                                       y_mean_te_site[0],\n",
    "                                                       y_var_te_site[0],\n",
    "                                                       yhat_mean_te_site[0],\n",
    "                                                       yhat_var_te_site[0],\n",
    "                                                       metrics_te_site['MSLL'][0],\n",
    "                                                       metrics_te_site['EXPV'][0],\n",
    "                                                       metrics_te_site['SMSE'][0],\n",
    "                                                       metrics_te_site['RMSE'][0],\n",
    "                                                       metrics_te_site['Rho'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save per site test set metrics variable to CSV file\n",
    "blr_site_metrics.to_csv('blr_site_metrics.csv', index=False, index_label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save overall test set metrics to CSV file\n",
    "blr_metrics.to_csv('blr_metrics.csv', index=False, index_label=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Interpreting model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output evaluation metrics definitions: \n",
    "* yhat - predictive mean\n",
    "* ys2 - predictive variance\n",
    "* nm - normative model\n",
    "* Z - deviance scores\n",
    "* Rho - Pearson correlation between true and predicted responses\n",
    "* pRho - parametric p-value for this correlation\n",
    "* RMSE - root mean squared error between true/predicted responses\n",
    "* SMSE - standardised mean squared error\n",
    "* EV - explained variance\n",
    "* MSLL - mean standardized log loss\n",
    "    * See page 23 in http://www.gaussianprocess.org/gpml/chapters/RW2.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
